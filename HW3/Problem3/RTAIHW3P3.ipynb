{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "amateur-minnesota",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/keith/.cache/torch/hub/ultralytics_yolov5_master\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  1    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  1    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
      "  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model Summary: 283 layers, 7276605 parameters, 7276605 gradients\n",
      "\n",
      "YOLOv5 ðŸš€ 2021-3-29 torch 1.8.1+cu111 CUDA:0 (GeForce GTX 1050 Ti, 4042.375MB)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding autoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "import trt_pose.coco\n",
    "from torch2trt import TRTModule\n",
    "import torchvision.transforms as transforms\n",
    "import PIL.Image\n",
    "import cv2\n",
    "from trt_pose.draw_objects import DrawObjects\n",
    "from trt_pose.parse_objects import ParseObjects\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "corresponding-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update model path if need be\n",
    "with open('trt_pose/tasks/human_pose/human_pose.json', 'r') as f:\n",
    "    human_pose = json.load(f)\n",
    "\n",
    "topology = trt_pose.coco.coco_category_to_topology(human_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caring-carrier",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#update model path if need be\n",
    "OPTIMIZED_MODEL = 'resnet18_baseline_att_224x224_A_epoch_249_trt.pth'\n",
    "\n",
    "model_trt = TRTModule()\n",
    "model_trt.load_state_dict(torch.load(OPTIMIZED_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "alike-brisbane",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda()\n",
    "device = torch.device('cuda')\n",
    "\n",
    "def preprocess(image):\n",
    "    global device\n",
    "    device = torch.device('cuda')\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device)\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "decent-missile",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_objects = ParseObjects(topology)\n",
    "draw_objects = DrawObjects(topology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "surrounded-appreciation",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 224\n",
    "HEIGHT = 224\n",
    "\n",
    "def poseProc(crop_img, w, h):\n",
    "    crop_img = cv2.resize(crop_img, (WIDTH,HEIGHT), interpolation =cv2.INTER_LINEAR)\n",
    "    data = preprocess(crop_img)\n",
    "    cmap, paf = model_trt(data)\n",
    "    cmap, paf = cmap.detach().cpu(), paf.detach().cpu()\n",
    "    counts, objects, peaks = parse_objects(cmap, paf)\n",
    "    draw_objects(crop_img, counts, objects, peaks)\n",
    "    crop_img = cv2.resize(crop_img, (w, h), interpolation =cv2.INTER_LINEAR)\n",
    "    return crop_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dominant-disco",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detPoseProc(video):\n",
    "    cap = cv2.VideoCapture(video)\n",
    "    \n",
    "    cv2.namedWindow('Human Detection', cv2.WINDOW_AUTOSIZE)\n",
    "    numFrame = 0\n",
    "    if cap.isOpened():\n",
    "        t0 = time.time()\n",
    "        torch.cuda.current_stream().synchronize()\n",
    "        while(cap.isOpened()):\n",
    "            ret, frame = cap.read()\n",
    "            if ret == False:\n",
    "                break\n",
    "            else:\n",
    "                numFrame += 1\n",
    "                #frame = cv2.resize(frame, (1280,720), interpolation =cv2.INTER_LINEAR)\n",
    "                results = model(frame)\n",
    "                \n",
    "                #iterate through detections\n",
    "                for p in range(0, len(results.xyxy[0])):\n",
    "                    \n",
    "                    #only do pose estimation if object is a person\n",
    "                    #0 is person class\n",
    "                    if(results.xyxy[0][p][5] < 1):\n",
    "                        x = int(results.xyxy[0][p][0])\n",
    "                        y = int(results.xyxy[0][p][1])\n",
    "                        w = int(results.xyxy[0][p][2] - results.xyxy[0][p][0])\n",
    "                        h = int(results.xyxy[0][p][3] - results.xyxy[0][p][1])\n",
    "                        img = frame.astype(np.uint8)\n",
    "                        crop_img = img[y:y+h, x:x+w]\n",
    "                        top_left = (x, y)\n",
    "                        bottom_right = (x + w, y + h)\n",
    "                        crop_img = poseProc(crop_img, w, h)\n",
    "                        \n",
    "                        #poor way to take cropped image and overlap onto original image\n",
    "                        frame[y:y+h, x:x+w] = crop_img\n",
    "                        frame = cv2.rectangle(frame, top_left, bottom_right, [0, 0, 255], 2)\n",
    "                    \n",
    "                        \n",
    "                cv2.imshow('Human Detection', frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break \n",
    "        torch.cuda.current_stream().synchronize()\n",
    "        t1 = time.time()\n",
    "    print(\"FPS: \" + str(numFrame / (t1 - t0)) )   \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "boring-cowboy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 21.51590127510741\n"
     ]
    }
   ],
   "source": [
    "#webcam implementation\n",
    "detPoseProc(0)\n",
    "\n",
    "#video\n",
    "#update model path if need be\n",
    "#detPoseProc('pedestrian.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-marks",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
